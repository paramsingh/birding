
## initial research

https://chatgpt.com/share/6805075a-5aec-8003-88e4-86dbebd047a8

# BirdCLEF+ 2025: Competition and Techniques Overview

## Competition Setup and Goals
- **BirdCLEF+ 2025** is a Kaggle code competition (part of LifeCLEF 2025) focused on identifying species from **audio soundscapes** recorded in the Middle Magdalena Valley of Colombia. Unlike prior BirdCLEF challenges that only featured birds, this edition includes **multiple taxonomic groups** (birds, frogs, insects, mammals – ~206 classes total) in the audio data ([BirdCLEF+ 2025 | ImageCLEF / LifeCLEF -  Multimedia Retrieval in CLEF](https://www.imageclef.org/BirdCLEF2025#:~:text=The%20broader%20goals%20for%20this,unlabeled%20data%20for%20improving%20detection%2Fclassification)). Participants must detect which species are calling within continuous recordings (typically several minutes long) under real-world conditions (multiple vocalizing species, background noise, etc.).
- The challenge emphasizes **low-resource species** and semi-supervised learning. Many target species are under-studied or **rare, with very limited labeled training clips available**, so models must generalize from few examples ([BirdCLEF+ 2025 | ImageCLEF / LifeCLEF -  Multimedia Retrieval in CLEF](https://www.imageclef.org/BirdCLEF2025#:~:text=Valley%20of%20Colombia%2FEl%20Silencio%20Natural,unlabeled%20data%20for%20improving%20detection%2Fclassification)). A key goal is to leverage **unlabeled audio** (provided from the same habitat) to improve recognition performance via techniques like unsupervised pre-training or pseudo-labeling ([BirdCLEF+ 2025 | ImageCLEF / LifeCLEF -  Multimedia Retrieval in CLEF](https://www.imageclef.org/BirdCLEF2025#:~:text=Valley%20of%20Colombia%2FEl%20Silencio%20Natural,unlabeled%20data%20for%20improving%20detection%2Fclassification)). Successful solutions will aid biodiversity monitoring by reliably detecting species from audio with minimal data, helping conservationists track restoration efforts in remote habitats.

## Data Preparation and Audio Preprocessing
- **Dataset composition:** Training data in past BirdCLEF competitions has consisted of thousands of short audio clips (bird call recordings from sources like Xeno-Canto) covering each species, while test data are long **soundscape recordings** (e.g. 5–10 minute continuous audio) in which species calls must be detected ([BirdCLEF 2023 | ImageCLEF / LifeCLEF -  Multimedia Retrieval in CLEF](https://www.imageclef.org/BirdCLEF2023#:~:text=solution%20with%20five%20rows%20of,used%20to%20train%20a%20classifier)) ([BirdCLEF 2023 | ImageCLEF / LifeCLEF -  Multimedia Retrieval in CLEF](https://www.imageclef.org/BirdCLEF2023#:~:text=recognize%20vocalizing%20birds%20in%20a,used%20to%20train%20a%20classifier)). For example, BirdCLEF 2024 provided 182 bird species with ~240k short clips for training, and ~1,100 unlabeled soundscape segments for testing (4 minutes each) ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=Competition%20host%20provide%20the%20random,utilize%20to%20augmentation%20or%20otehrs)) ([BirdCLEF+ 2025 | ImageCLEF / LifeCLEF -  Multimedia Retrieval in CLEF](https://www.imageclef.org/BirdCLEF2025#:~:text=,unlabeled%20data%20for%20improving%20detection%2Fclassification)). In BirdCLEF+ 2025, the training set similarly mixes short curated clips for each species and unlabeled soundscapes from the target Colombian reserve to exploit.
- **Audio preprocessing:** A common approach is to convert waveform audio into a time–frequency representation such as a **mel-spectrogram**. Participants typically resample audio to a fixed rate (e.g. 32 kHz) and compute log-mel spectrograms with a sliding window FFT (e.g. 1024–2048 samples) and ~128 mel frequency bands ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=SED%20EfficientNetV2,2048%20627)). The spectrogram parameters can be tuned per model – top teams often vary the spectrogram resolution across models to capture different time–frequency scales (e.g. using different FFT hop lengths or frequency ranges) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=SED%20EfficientNetV2,2048%20627)). Some also append **delta and delta-delta** (first/second derivative) channels to the spectrogram to provide the model with local temporal gradients ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=summed%20up%20to%20one%2010,noise%20to%20the%20audio%20signal)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=Deltas%20and%20Delta,density%20inversely%20proportional%20to%20frequency)). These spectrogram “images” (often sized around 224×224 or 256×256) become the input to CNN or transformer models, analogous to processing an image ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=For%20each%20record%20we%20use,image%20size%20of%20224x224)).
- **Slicing long audio:** Because test soundscapes are long, models analyze them in shorter frames (e.g. 5-second windows). A typical pipeline is to **split audio into overlapping 5–10s segments** and generate a spectrogram for each segment. For example, BirdCLEF 2023 required classifying each 5-second chunk of audio ([BirdCLEF 2023 | ImageCLEF / LifeCLEF -  Multimedia Retrieval in CLEF](https://www.imageclef.org/BirdCLEF2023#:~:text=solution%20with%20five%20rows%20of,used%20to%20train%20a%20classifier)). Sliding-window inference (with overlap) ensures calls that span segment boundaries are caught. One winning solution predicted on 5s frames with a 2.5s offset as a form of test-time augmentation, then merged the results ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=Image)). In training, random 5–30s excerpts are cropped from each training clip to augment and simulate the short context available during inference ([Notebook for the <Cornell Lab of Ornithology> Lab at CLEF 2024](https://ceur-ws.org/Vol-3740/paper-204.pdf#:~:text=Data%20preprocessing%20involves%20transforming%20audio,on%20the%20BirdCLEF%202024%20leaderboard)) ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=)).

## Data Augmentation and Balancing
- **Audio augmentation:** To tackle the noisy, complex nature of field recordings, competitors apply heavy augmentation to training audio. Common techniques include adding various types of **noise** (Gaussian white noise, pink noise, etc.), applying random **gain** (volume changes), **pitch shifting** of calls, and **time shifting** (phase shifting the audio in time) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=follows%3A%201,of%20the%20audio%20signal%20without)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=4,uses%20the%20formula%20as%20follows)). Many solutions also incorporate **background noise injection**, i.e. overlaying segments of real background ambiance (wind, rain, insects, or crowd noise) onto training clips so models learn to focus on the bird/call signal ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=frequency,changing%20its%20pitch%20or%20duration)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=4,uses%20the%20formula%20as%20follows)). For example, a 2024 team extracted ~50 pure background snippets from unlabeled soundscapes and mixed them into training audio as augmentation ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=)). This improved robustness to environmental noise.
- **Mixing and multi-label augmentation:** Because in soundscapes multiple species may call simultaneously, top teams simulate this by **mixing training samples**. Techniques like Mixup and Cutmix are adapted to audio: two clips are overlapped, and the target labels are combined (taking the union or max of one-hot labels) ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=For%20each%20record%20we%20use,image%20size%20of%20224x224)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=duration,spectrograms)). One winning approach used an “OR-mixup” for audio, where overlapping two samples yields a label indicating any species that appeared in either clip ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=altering%20its%20duration,samples%20and%20%F0%9D%9C%8F%20is%20Mixup)). This trains the model for polyphonic (multi-species) detection. Teams also perform SpecAugment on the spectrograms (randomly masking frequency bands or time intervals) to further diversify the data ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=ratio.%20For%20Mel,spectrograms)).
- **Class imbalance handling:** Rare species have very few training examples, so methods to rebalance the data are crucial. Participants often use **class-balanced sampling** (or weighted loss) so that each species is seen more equally during training ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=training%20strategies,4%29%2C%20class%20balanced)). For instance, BirdCLEF 2024’s 1st-place team capped the maximum clips per species to 500 for common species (to avoid over-representing abundant classes) and oversampled the scarcest species ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=some%20extra%20samples%20to%20fight,class%20imbalance)). Others use **sample weights** or upsample classes below a threshold – one team ensured any species with <15 clips was duplicated until it had 15 in each epoch ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=%2A%20Applied%20stratified%205,to%2015%20samples%20during%20training)). This mitigates the long-tailed distribution. Additionally, external data is brought in for rare classes when possible (e.g. pulling extra recordings from Xeno-Canto for those species) ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=,to%2015%20samples%20during%20training)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=To%20further%20expand%20the%20dataset,about%20119000%20covering%20834%20species)).
- **External data and pretraining:** Building on past BirdCLEF editions, it’s common to leverage prior competition datasets and public archives. Top teams aggregate birdcall audio from **previous years’ BirdCLEF** competitions and Xeno-Canto to pre-train models on a broader set of species before fine-tuning on the 2025 species. For example, a 2023 solution incorporated **21,000 additional clips from Xeno-Canto** (on top of ~16k provided) and even included BirdCLEF 2020–2022 audio in a pre-training stage, assembling a dataset of 119k clips covering 834 species ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=As%20in%20previous%20BirdCLEF%20challenges%2C,target%20species%20only%20appear%20in)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=community,about%20119000%20covering%20834%20species)). Models pre-trained on this large corpus (e.g. via multi-label classification on all bird species) can then be fine-tuned to the target 200+ classes, giving a head start especially for feature extraction of bird sounds.

## Model Architectures and Feature Extraction
- **CNNs on spectrograms:** The workhorses of BirdCLEF solutions are **convolutional neural networks** applied to 2D spectrogram inputs. Participants often use architectures adapted from image recognition – e.g. EfficientNet, ResNet, MobileNet, ConvNeXt – treating the mel-spectrogram as a pseudo-image (time axis vs frequency axis) ([Notebook for the <Cornell Lab of Ornithology> Lab at CLEF 2024](https://ceur-ws.org/Vol-3740/paper-204.pdf#:~:text=employing%20an%20ensemble%20of%20EfficientNet,The%20ensemble%20strategy%2C%20combining)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=SED%20EfficientNetV2,2048%20627)). These CNN models excel at learning frequency-time patterns of bird calls. Many teams found EfficientNet family models (B0/B1 in particular) hit a sweet spot of accuracy vs. speed, especially important due to the Kaggle CPU inference constraint ([Notebook for the <Cornell Lab of Ornithology> Lab at CLEF 2024](https://ceur-ws.org/Vol-3740/paper-204.pdf#:~:text=the%20current%20competition%E2%80%99s%20data%2C%20and,My%20results)) ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=)). For instance, an EfficientNet-B0 trained from scratch on BirdCLEF 2024 data and an EfficientNet-B1 pre-trained on prior BirdCLEF data were ensembled to good effect ([Notebook for the <Cornell Lab of Ornithology> Lab at CLEF 2024](https://ceur-ws.org/Vol-3740/paper-204.pdf#:~:text=employing%20an%20ensemble%20of%20EfficientNet,Mel%20spectrograms%2C%20optimized%20through%20feature)). Similarly, lightweight networks (MobileNetV3, MNASNet, etc.) have been used to meet the strict 2-hour runtime limit while still capturing spectro-temporal features ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=This%20pipeline%20uses%20a%20variety,on%20224x224%20log%20mel%20spectrograms)).
- **Audio event detection models:** Rather than treating each 5s clip as an independent image classification, some top solutions use **sound event detection (SED)** architectures that account for temporal structure within the clip. One approach is a CNN with an **attention or pooling head over time**: the CNN processes the spectrogram to produce a time × frequency feature map, then an attention mechanism (or a global pooling like GeM) aggregates information over the time dimension ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=3,birds%20over%20the%20time%20dimension)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=Figure%201%3A%20Model%20Architecture%20of,layer%20implemented%20to%20apply%20pooling)). This yields a model that can localize which part of the clip a bird call occurs in, essentially focusing on the central frames while using the surrounding audio as context ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=3,birds%20over%20the%20time%20dimension)). For example, a 2nd-place 2023 team trained a 10-second SED model with an attention head, which at inference looked at 10s windows and predicted bird presence in the central 5s segment ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=and%20process%20time%20and%20frequency,birds%20over%20the%20time%20dimension)). Such architectures (often inspired by detection or segmentation models) help handle clips where a species calls only briefly.
- **Transformer and hybrid models:** While CNNs dominate, recent efforts have explored transformer-based models for audio. Some teams experimented with **Vision Transformers (ViT)** and **audio spectrogram transformers** to capture global time-frequency relationships. In 2024, EfficientViT (a fast vision transformer) models were used alongside CNNs – one team noted EfficientViT-B0 yielded strong accuracy while still being fast on CPU, so they ensembled it with CNNs for a boost ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=This%20pipeline%20uses%20a%20variety,on%20224x224%20log%20mel%20spectrograms)). A few approaches have also tried sequence models on raw audio or tokenized audio: for instance, representing audio via learned discrete codes (using an autoencoder like EnCodec) and feeding that sequence into a transformer. However, the winning solutions in 2023–2024 largely stuck to CNN backbones or hybrids (CNN feature extractor + transformer-style attention head). The complexity of long audio and computational limits made pure transformers less common, but this remains an area of potential (e.g. Transformers could integrate longer context than 5s clips).
- **Raw waveform models:** Nearly all top solutions rely on spectrograms, but there have been attempts to learn directly from raw waveforms. One 2024 team built a **1D CNN** taking raw audio: they downsampled 5-second clips to 16 kHz and reshaped the waveform (80k samples) into a 2D tensor (625×128) to input into a CNN ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=)). Essentially, they treated 128-sample time slices as “pseudo-channels” and used convolution to learn features from the raw signal. This model, coupled with a specialized head for event detection, performed competitively when ensembled with spectrogram-based models ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=)). In general, raw-signal models are less common (they require learning frequency filters from scratch), but they can complement mel-based models since they might capture different nuances. The best entries often ensemble a variety of model types to capitalize on this.

## Training & Semi-Supervised Strategies
- **Transfer learning and fine-tuning:** Given the limited examples for many species, most competitors do **multi-stage training**. First, models are trained (or pre-trained) on a broad dataset – e.g. all available data from prior years or a large subset including extra species – to learn general audio features. Then a fine-tuning is done on the specific BirdCLEF 2025 species, often using a smaller learning rate and possibly a different loss focusing on those classes ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=community,about%20119000%20covering%20834%20species)). For instance, a 2023 team pre-trained a ResNet on 834 bird species (previous competitions) with a cross-entropy loss, then fine-tuned it on the 264 species of that year using binary cross-entropy for multi-label classification ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=set%20for%20each%20model,Inference%20Acceleration%20using%20OpenVINO)). This two-step approach (sometimes called **pretrain on superset, then fine-tune on target**) consistently appears in top solutions, as it boosts performance on rare classes and speeds up convergence ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=community,about%20119000%20covering%20834%20species)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=set%20for%20each%20model,Inference%20Acceleration%20using%20OpenVINO)).
- **Loss functions and class weighting:** Bird audio classification is a multi-label problem (multiple species can be present in one clip). The standard loss is **binary cross-entropy (BCE)** applied to each of the N species outputs. However, some found benefits in alternative losses – e.g. focal loss to down-weight easy negatives, or even using a softmax cross-entropy during pretraining when forcing a single label assumption, then switching to BCE ([Notebook for the <Cornell Lab of Ornithology> Lab at CLEF 2024](https://ceur-ws.org/Vol-3740/paper-204.pdf#:~:text=the%20current%20competition%E2%80%99s%20data%2C%20and,My%20results)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=set%20for%20each%20model,Inference%20Acceleration%20using%20OpenVINO)). One 2023 winning solution first trained with a normal cross-entropy (treating it like multi-class) which helped the model learn general features, then fine-tuned with BCE which is the proper multi-label loss ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=set%20for%20each%20model,Inference%20Acceleration%20using%20OpenVINO)). Additionally, **class-balanced loss** weighting is used: the 2023 champions mention applying per-class weights inversely proportional to class frequency (to handle the imbalance) ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=)). Overall, careful loss scheduling (e.g. change loss function or weighting partway through training) can yield small but meaningful gains in rare-species recall.
- **Cross-validation and validation strategy:** Because the test set consists of soundscapes (not individual call clips) and the distribution of species differs from training, validation is tricky. Top teams create custom validation schemes: for example, **grouped k-fold CV** where splits are done by geographic location or by audio recorder to ensure train/val have different environments ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=,to%2015%20samples%20during%20training)). This prevents overfitting to a particular background. Some also construct **synthetic soundscape validation sets** – e.g. by mixing isolated bird call clips into real background noise clips to mimic test conditions ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=The%20authors%20switched%20to%20a,synthetic%20data%20approach%20for%20validation)). In 2024, one team curated a validation set by taking 40 species’ clips and overlaying them on segments of unlabeled soundscape background, creating mock 10-minute soundscapes with known truth, and used that to calculate validation AUC ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=The%20authors%20switched%20to%20a,synthetic%20data%20approach%20for%20validation)). This helped tune models for detection threshold and evaluate performance on multi-species scenarios.

- **Semi-supervised learning (pseudo-labeling):** A crucial strategy, given the abundance of unlabeled recordings, is to use **pseudo-labels** – i.e. have your model label the unlabeled soundscapes and then retrain on those predictions. Many high-ranking teams in 2024 did iterative pseudo-labeling. **First, train a “level-0” model on the labeled data**, then run it on the unlabeled audio to infer where each species may be calling. Those predictions (with confidence filtering) are added to the training set for a **second training stage** ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=Our%20pipeline%20is%20summarized%20below,used%20for%20the%20final%20submission)). This effectively increases training data, especially for species that had few real labels. An ensemble from one team’s first stage achieved about 0.61 AUC on private LB; after adding pseudo-labeled examples and retraining a new model, their final ensemble reached ~0.69 AUC – a huge jump (see figure) ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=Our%20pipeline%20is%20summarized%20below,used%20for%20the%20final%20submission)).
   ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024)) *Figure: Multi-stage pseudo-labeling pipeline used by a BirdCLEF 2024 team. An initial model (Level 0) is trained on limited real data (plus some extra gathered clips). It generates pseudo-labels on unlabeled soundscapes, which are then used to train a stronger Level 1 model. This approach significantly improved the private leaderboard score (from ~0.61 to ~0.69 AUC) ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=Our%20pipeline%20is%20summarized%20below,used%20for%20the%20final%20submission)).*
- **Best practices for pseudo-labeling:** When adding pseudo-labeled data, top teams take care to maintain quality and balance. Pseudo-labels might be filtered by confidence (e.g. only add predictions with probability > 0.9 for rare species to avoid noise). Also, the **mix of real vs. pseudo data** is important – one winning team found using a large batch size and about a 1:1 ratio of real to pseudo examples in each batch worked well ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=)). They also randomized a volume amplification factor on pseudo clips so the model didn’t overfit to the audio amplitude of its own predictions ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=the%20given%20time%20frame%20varied,parts%20are%20masked%20before%20averaging)) ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=0%20,because%20public%20leaderboard%20score%20did)). Essentially, pseudo-labeling is done in one or two waves; further iterations yield diminishing returns or can even introduce error accumulation (teams noted that by the 3rd iteration, they had to normalize prediction scores as they got too overconfident) ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=0%20,because%20public%20leaderboard%20score%20did)). When done right, however, leveraging the **unlabeled field recordings via pseudo-labels has proven to significantly boost model generalization**, making it a key technique for BirdCLEF+ 2025.

## Ensembling & Inference Techniques
- **Model ensembling:** As with many Kaggle competitions, ensembling is used extensively to squeeze out extra performance. Top BirdCLEF submissions combine **multiple models trained with different architectures and inputs**. For example, a 2024 4th-place team ensembled three model “types” – *Model A:* a CNN on mel-spectrogram (inspired by a 2023 approach), *Model B:* another mel-CNN with a different architecture (Inception-based), and *Model C:* a raw-waveform CNN – and blended their outputs for a higher overall AUC ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=Summary%20of%20solution%20is%20here)) ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=%2A%20inception)). Common ensemble strategies are averaging or weighted averaging of predicted probabilities for each species ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=Post,44)). Some teams assign higher weight to models that perform better on certain species or use a geometric mean fusion for a slight tweak ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=%600.15,0.5)). The diversity in model types (e.g. one may handle background noise better, another picks up faint calls) often yields a **more robust ensemble** than any single model. In 2023, the winning solution was reportedly an ensemble of several large CNN backbones (ConvNeXt, EfficientNet, NFNet, etc.) that outperformed any individual network.
- **Test-time augmentation (TTA):** To improve inference, participants apply TTA on audio data. The primary form of TTA is using **overlapping context windows**, as mentioned earlier – e.g. if the model expects 5-second segments, one can run it on frames [0–5s], [2.5–7.5s], [5–10s], etc., then average the predictions for each timeframe. This was shown to catch intermittent calls that might start just at the edge of a window ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=Image)). Another TTA trick is to adjust audio playback speed slightly (effectively pitch shifting) at inference to see if the prediction is consistent, though this is less common due to time limits. Some vision-based TTAs (like horizontal flipping of spectrogram, which corresponds to time-reversal of audio) are generally not meaningful for audio. Instead, ensembles cover the augmentation aspect by having models trained with different spectrogram parameters (which is like a multi-scale TTA).
- **Post-processing of predictions:** After obtaining frame-wise predictions for a soundscape, teams often implement smoothing or thresholding logic to refine the results. A simple but effective post-process is **temporal smoothing**: e.g. take each 5s window prediction and also add a portion of the neighbor windows’ predictions, on the assumption that a bird calling will likely continue in adjacent segments ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=from%20sigmoid%20outputs,test%20set%20are%20preprocessed%20in)). One 2024 solution summed each window’s probabilities with 0.5× the probabilities of the preceding and following window to reduce false negatives (if a call spills over boundary) ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=from%20sigmoid%20outputs,test%20set%20are%20preprocessed%20in)) ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=given%20time%20window%20are%20summed,calculated)). Another consideration is to suppress very short detections – e.g. if a species is predicted in one 5s segment but not in any neighbor, one might drop it as an isolated likely false alarm. Given the metric (macro-AUC or F1-based), most teams focus on getting the ranking of probabilities right rather than applying hard thresholds. However, ensuring that a species prediction is consistent over a few consecutive frames can improve precision.
- **Inference optimization:** BirdCLEF competitions impose a strict inference time limit (often <= 2 hours on a Kaggle CPU for processing ~1000+ minutes of audio). To meet this, **efficient inference pipelines** are essential. Techniques include: pre-computing features, model quantization, and parallelization. Many top teams convert their PyTorch models to **ONNX/OpenVINO** format and use INT8 quantization to speed up inference without much loss in accuracy ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=6,Training%20Quantization)). For instance, the 2024 4th-place team quantized their models (except the final layers) and achieved a ~30–40% speedup, which was critical to run their ensemble within the time limit ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=The%20authors%20used%20OpenVINO%20and,up%20their%20model%27s%20inference%20time)). Additionally, reading and processing audio is I/O heavy, so solutions use **multi-threading and caching** – e.g. load and resample audio in parallel, compute the mel-spectrogram once per soundscape and reuse it for all model predictions to avoid duplicate work ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=To%20speed%20up%20inference%2C%20audio,create%20predictions%20for%20all%201100)). By combining these optimizations, winners manage to deploy ensembles of up to 5–7 models under the time cap. It’s important for 2025 participants to budget inference time per clip and test on the provided CPU to ensure their solution is not only accurate but also efficient.

## Notable Past Solutions and Resources
- **BirdCLEF 2024 – Western Ghats (India):** The top teams in 2024 devised methods directly relevant to 2025’s low-resource setup. The winning team (“Team Kefir”) achieved ~69.0% ROC-AUC on the private leaderboard ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=)) by ensembling multiple EfficientNet and ViT models and leveraging extensive pseudo-labeling on unlabeled soundscapes. Another team (CPMP, Henkel et al.) introduced a two-stage training pipeline with pseudo-labels (see figure above) which significantly boosted rare-species detection ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=Our%20pipeline%20is%20summarized%20below,used%20for%20the%20final%20submission)). They report that adding a volume of pseudo-labeled examples *comparable to the original training set* and retraining gave a **+8% AUC improvement**, nearly matching the first place ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=Our%20pipeline%20is%20summarized%20below,used%20for%20the%20final%20submission)) ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=0%20,because%20public%20leaderboard%20score%20did)). Most 2024 solutions also incorporated additional Xeno-Canto data and prior BirdCLEF datasets, heavy spectrogram augmentation, and careful model ensembling, which are all likely to be essential in 2025. Detailed write-ups of these approaches can be found in the LifeCLEF 2024 working notes (e.g. Team adsr’s 2nd-place paper describes their augmentation and ensemble strategies in depth) ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=ensure%20completion%20within%20the%202,parts%20are%20masked%20before%20averaging)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=follows%3A%201,10%2C%2011)).
- **BirdCLEF 2023 – Eastern Africa (Kenya):** In 2023, with 264 species to recognize, the first-place solution (by Volodymyr Sydorskyy et al.) emphasized **data quality and strong models** – their summary was titled "Correct Data is All You Need", underscoring meticulous curation of training data and labels. They combined several high-capacity CNNs (such as ConvNeXt and NFNet backbones pre-trained on ImageNet21k) into an ensemble, and this brute-force approach led to the top score. The second-place team (“Accenture Japan – Lihang Hong”) focused on an **EfficientNet-based SED model** with extensive data augmentation and background noise addition ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=frequency,changing%20its%20pitch%20or%20duration)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=3,birds%20over%20the%20time%20dimension)). They also utilized **OpenVINO acceleration**, converting seven CNN models to OpenVINO IR and averaging their outputs, which allowed them to meet the 2h inference limit and secure 2nd place ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=establish%20a%20reliable%20classification%20model,Convolutional%20Neural%20Network%2C%20Sound%20Event)) ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=In%20inference%20process%2C%20we%20generated,frameworks%2C%20such%20as%20TensorFlow%2C%20Caffe)). Notably, many 2023 competitors pre-trained on previous years’ data and used mixup strategies, which carried into 2024. The lessons from 2023 showed the importance of ensemble diversity (CNN + SED models, different spectrogram settings) and of handling label imbalance – the 2023 winners introduced a novel class-weighting scheme that was adopted by teams in 2024 ([〖Kaggle〗BIrdCLEF2024 4th Solution explained](https://zenn.dev/yuto_mo/articles/86e2db028e141c#:~:text=)).
- **Resources:** Kaggle’s discussion forums and released code contain a wealth of practical insight. For instance, the **open-source codebases of winning solutions** have been published – the 2023 1st place team shared their full training pipeline and inference scripts on GitHub ([GitHub - VSydorskyy/BirdCLEF_2023_1st_place: Codebase for BirdClef 2023 solution](https://github.com/VSydorskyy/BirdCLEF_2023_1st_place#:~:text=Bird%20Clef%202023%201st%20place,solution)), and a 2024 winning team has a public repo outlining their pseudo-labeling approach and model configurations ([GitHub - jfpuget/birdclef-2024: Solution to Birdclef 2024 challenge on Kaggle](https://github.com/jfpuget/birdclef-2024#:~:text=birdclef)). Participants should review these if available, as they illustrate how to handle real competition constraints (data processing, inference optimization, etc.). Additionally, the LifeCLEF workshop papers (CEUR proceedings) for BirdCLEF 2023 and 2024 detail top methods and provide ablation studies ([](https://ceur-ws.org/Vol-3497/paper-172.pdf#:~:text=establish%20a%20reliable%20classification%20model,Convolutional%20Neural%20Network%2C%20Sound%20Event)) ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=0%20,because%20public%20leaderboard%20score%20did)). These resources underscore best practices like: use of **audiomentations** library for noise augmentation, how to perform spectrogram extraction with **TorchAudio/Librosa**, efficient batching on CPU, and even tricks like a timer to stop inference before the hard limit to avoid failure ([Improving Bird Recognition using Pseudo-Labeled Recordings from the Target Location](https://ceur-ws.org/Vol-3740/paper-199.pdf#:~:text=Due%20to%20variations%20in%20the,Predictions)). By studying these solutions and techniques, a BirdCLEF+ 2025 participant can craft a strong strategy that combines advanced audio processing, modern deep learning models, and pragmatic engineering to competitively tackle the complex task of species identification in soundscapes.
